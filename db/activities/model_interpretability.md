---
name: "Model Interpretability"
id: model_interpretability
startTime: October 18, 2020 12:00:00-500
endTime: October 18, 2020 13::15-500
duration: 60
mediaType: embed_url
eventId: 5f1bc49319f52100035dda03
mediaLink: https://www.youtube.com/embed/cbQDSWuHNv8
thumbnail: https://mcusercontent.com/36d73585139760aa245837bb2/images/907ef44b-9d6c-478c-baa4-5ef79cbb9901.jpeg
presenter: Alan Feder
presenterAbout: Alan Feder is a Principal Data Scientist currently working for Invesco Mutual Funds. Prior to Invesco, Alan worked for 8 years gaining actuarial and data science experience in finance. Alan has a Bachelors Degree in Mathematics & Economics from Columbia University, and a Masters in Statistics from Columbia as well.
presenterSocials:
  - type: Alan's LinkedIn
    link: https://www.linkedin.com/in/alanfeder/
priority: 7
relatedActivities: null
---

In this session on model interpretability, we will do a deep dive into ways to analyze why machine learning models make certain decisions.

But why is model interpretability so important? Imagine that the CDC develops an antiviral drug that is speculated to be a cure for the Covid-19 virus. Based on a few trials, the CDC develops a predictive model that can correctly predict the success of the drug on a new patient 90 percent of the time.

But, is just knowing the success rate enough to trust the model without caring about why it made a certain decision? What if the drug has serious negative side-effects on people with heart diseases?

In this session, we will cover a few of the necessary tools such as feature importance plots, partial dependence plots, and tree interpreters to answer such questions. We will walk through the code associated with these concepts on a dataset from Kaggle.

#### Useful Links

- [Watch the trailer for this class](https://www.youtube.com/watch?v=O4w0cE2rmTk)
- [Tutorial Materials](https://github.com/sheelabhadra/TAMU-Datathon-Model-Interpretability)
